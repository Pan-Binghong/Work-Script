{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>算力卡型号</th>\n",
       "      <th>制造商</th>\n",
       "      <th>架构</th>\n",
       "      <th>FP64</th>\n",
       "      <th>FP64 Tensor Core</th>\n",
       "      <th>FP32</th>\n",
       "      <th>TF32 Tensor Core</th>\n",
       "      <th>BFLOAT16 Tensor Core</th>\n",
       "      <th>FP16 Tensor Core</th>\n",
       "      <th>INT8 Tensor Core</th>\n",
       "      <th>INT4 Tensor Core</th>\n",
       "      <th>显存大小 | 类型</th>\n",
       "      <th>显存带宽</th>\n",
       "      <th>接口类型</th>\n",
       "      <th>卡间互连</th>\n",
       "      <th>TDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4090</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>Ada Lovelace</td>\n",
       "      <td>1.3 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>83 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>330 TFLOPS</td>\n",
       "      <td>660 TOPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24GB GDDR6X</td>\n",
       "      <td>1000GB/s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4090D</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>Ada Lovelace</td>\n",
       "      <td>1.1 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>74 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>300 TFLOPS</td>\n",
       "      <td>600 TOPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24GB GDDR6X</td>\n",
       "      <td>1000GB/s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>425W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A2</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>4.5 TFLOPS</td>\n",
       "      <td>9 TFLOPS | 18TFLOPS*</td>\n",
       "      <td>18 TFLOPS | 36 TFLOPS*</td>\n",
       "      <td>18 TFLOPS | 36 TFLOPS*</td>\n",
       "      <td>36 TOPS | 72 TOPS*</td>\n",
       "      <td>72 TOPS | 144 TOPS*</td>\n",
       "      <td>16GB GDDR6</td>\n",
       "      <td>200GB/s</td>\n",
       "      <td>PCIe Gen4 x8</td>\n",
       "      <td>PCIe Gen4 x8</td>\n",
       "      <td>40-60W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A10</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>Ampere</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>31.2 TFLOPS</td>\n",
       "      <td>62.5 TFLOPS | 125 TFLOPS*</td>\n",
       "      <td>125 TFLOPS | 250 TFLOPS*</td>\n",
       "      <td>125 TFLOPS | 250 TFLOPS*</td>\n",
       "      <td>250 TOPS | 500 TOPS*</td>\n",
       "      <td>500 TOPS | 1000 TOPS*</td>\n",
       "      <td>24GB GDDR6</td>\n",
       "      <td>600GB/s</td>\n",
       "      <td>PCIe Gen4</td>\n",
       "      <td>PCIe Gen4 64GB/s</td>\n",
       "      <td>150W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A30</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>Ampere</td>\n",
       "      <td>5.2 TFLOPS</td>\n",
       "      <td>10.3 TFLOPS</td>\n",
       "      <td>10.3 TFLOPS</td>\n",
       "      <td>82 TFLOPS | 165 TFLOPS*</td>\n",
       "      <td>165 TFLOPS | 330 TFLOPS*</td>\n",
       "      <td>165 TFLOPS | 330 TFLOPS*</td>\n",
       "      <td>330 TOPS | 661 TOPS*</td>\n",
       "      <td>661 TOPS* | 1321 TOPS*</td>\n",
       "      <td>24GB HBM2</td>\n",
       "      <td>933GB/s</td>\n",
       "      <td>PCIe Gen4</td>\n",
       "      <td>PCIe Gen4: 64GB/s\\nThird-gen NVIDIA® NVLINK®\\n...</td>\n",
       "      <td>165W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>59</td>\n",
       "      <td>MLU270-F4</td>\n",
       "      <td>寒武纪</td>\n",
       "      <td>MLUv02</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>128 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>16 DDR4</td>\n",
       "      <td>102GB/s</td>\n",
       "      <td>PCIe 3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>60</td>\n",
       "      <td>DCU Z100</td>\n",
       "      <td>海光</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>10.1 TFLOPS</td>\n",
       "      <td>10.1 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>20.2 TFLOPS</td>\n",
       "      <td>40.5 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>32GB HBM2</td>\n",
       "      <td>1024GB/s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>61</td>\n",
       "      <td>DCU K100-AI</td>\n",
       "      <td>海光</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>49 TFLOPS</td>\n",
       "      <td>98 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>196 TFLOPS</td>\n",
       "      <td>392 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>64GB</td>\n",
       "      <td>1024GB/s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>62</td>\n",
       "      <td>N100</td>\n",
       "      <td>沐曦曦思</td>\n",
       "      <td>GPGPU+DLA</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>16GB HBM2e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PCIe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>63</td>\n",
       "      <td>C500</td>\n",
       "      <td>沐曦曦思</td>\n",
       "      <td>GPGPU+DLA</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>15vector/30matrix</td>\n",
       "      <td>140 TFLOPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280 TFLOPS</td>\n",
       "      <td>560 TFLOPS</td>\n",
       "      <td>-</td>\n",
       "      <td>64GB HBM2e</td>\n",
       "      <td>1843GB/s</td>\n",
       "      <td>PCIe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    编号        算力卡型号     制造商            架构        FP64 FP64 Tensor Core  \\\n",
       "0    1         4090  NVIDIA  Ada Lovelace  1.3 TFLOPS                -   \n",
       "1    2        4090D  NVIDIA  Ada Lovelace  1.1 TFLOPS                -   \n",
       "2    3           A2  NVIDIA             -           -                -   \n",
       "3    4          A10  NVIDIA        Ampere           -                -   \n",
       "4    5          A30  NVIDIA        Ampere  5.2 TFLOPS      10.3 TFLOPS   \n",
       "..  ..          ...     ...           ...         ...              ...   \n",
       "61  59    MLU270-F4     寒武纪        MLUv02           -                -   \n",
       "62  60     DCU Z100      海光             -           -                -   \n",
       "63  61  DCU K100-AI      海光             -           -                -   \n",
       "64  62         N100    沐曦曦思     GPGPU+DLA           -                -   \n",
       "65  63         C500    沐曦曦思     GPGPU+DLA           -                -   \n",
       "\n",
       "                 FP32           TF32 Tensor Core      BFLOAT16 Tensor Core  \\\n",
       "0           83 TFLOPS                          -                         -   \n",
       "1           74 TFLOPS                          -                         -   \n",
       "2          4.5 TFLOPS       9 TFLOPS | 18TFLOPS*    18 TFLOPS | 36 TFLOPS*   \n",
       "3         31.2 TFLOPS  62.5 TFLOPS | 125 TFLOPS*  125 TFLOPS | 250 TFLOPS*   \n",
       "4         10.3 TFLOPS    82 TFLOPS | 165 TFLOPS*  165 TFLOPS | 330 TFLOPS*   \n",
       "..                ...                        ...                       ...   \n",
       "61                  -                          -                         -   \n",
       "62        10.1 TFLOPS                10.1 TFLOPS                         -   \n",
       "63          49 TFLOPS                  98 TFLOPS                         -   \n",
       "64                  1                        NaN                       NaN   \n",
       "65  15vector/30matrix                 140 TFLOPS                       NaN   \n",
       "\n",
       "            FP16 Tensor Core      INT8 Tensor Core        INT4 Tensor Core  \\\n",
       "0                 330 TFLOPS              660 TOPS                     NaN   \n",
       "1                 300 TFLOPS              600 TOPS                     NaN   \n",
       "2     18 TFLOPS | 36 TFLOPS*    36 TOPS | 72 TOPS*     72 TOPS | 144 TOPS*   \n",
       "3   125 TFLOPS | 250 TFLOPS*  250 TOPS | 500 TOPS*   500 TOPS | 1000 TOPS*   \n",
       "4   165 TFLOPS | 330 TFLOPS*  330 TOPS | 661 TOPS*  661 TOPS* | 1321 TOPS*   \n",
       "..                       ...                   ...                     ...   \n",
       "61                         -            128 TFLOPS                       -   \n",
       "62               20.2 TFLOPS           40.5 TFLOPS                       -   \n",
       "63                196 TFLOPS            392 TFLOPS                       -   \n",
       "64                       NaN                   NaN                       -   \n",
       "65                280 TFLOPS            560 TFLOPS                       -   \n",
       "\n",
       "      显存大小 | 类型      显存带宽          接口类型  \\\n",
       "0   24GB GDDR6X  1000GB/s           NaN   \n",
       "1   24GB GDDR6X  1000GB/s           NaN   \n",
       "2    16GB GDDR6   200GB/s  PCIe Gen4 x8   \n",
       "3    24GB GDDR6   600GB/s     PCIe Gen4   \n",
       "4     24GB HBM2   933GB/s     PCIe Gen4   \n",
       "..          ...       ...           ...   \n",
       "61      16 DDR4   102GB/s      PCIe 3.0   \n",
       "62    32GB HBM2  1024GB/s           NaN   \n",
       "63         64GB  1024GB/s           NaN   \n",
       "64   16GB HBM2e       NaN          PCIe   \n",
       "65   64GB HBM2e  1843GB/s          PCIe   \n",
       "\n",
       "                                                 卡间互连     TDP  \n",
       "0                                                   -     NaN  \n",
       "1                                                   -    425W  \n",
       "2                                        PCIe Gen4 x8  40-60W  \n",
       "3                                    PCIe Gen4 64GB/s    150W  \n",
       "4   PCIe Gen4: 64GB/s\\nThird-gen NVIDIA® NVLINK®\\n...    165W  \n",
       "..                                                ...     ...  \n",
       "61                                                NaN    150W  \n",
       "62                                                NaN    250W  \n",
       "63                                                NaN    350W  \n",
       "64                                                NaN     70W  \n",
       "65                                                NaN     NaN  \n",
       "\n",
       "[66 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('computing_gpu.csv', encoding='utf-8')\n",
    "\n",
    "# 算力卡型号\n",
    "model_number = None\n",
    "\n",
    "# 制作商\n",
    "makers = None\n",
    "\n",
    "# FP64算力\n",
    "fp64_compute_power = None\n",
    "\n",
    "# FP32算力\n",
    "fp32_compute_power = None\n",
    "\n",
    "# TF32算力\n",
    "tf32_compute_power = None\n",
    "\n",
    "# FP16算力\n",
    "fp16_compute_power = None\n",
    "\n",
    "# INT8算力\n",
    "int8_compute_power = None\n",
    "\n",
    "# 显存大小\n",
    "memory_size = None\n",
    "\n",
    "# 显存带宽\n",
    "memory_bandwidth = None\n",
    "\n",
    "# 卡间互连\n",
    "inter_card_interconnect = None\n",
    "\n",
    "# 显存类型\n",
    "memory_type = None\n",
    "\n",
    "# 显存带宽\n",
    "memory_bandwidth = None\n",
    "\n",
    "json_template = {\n",
    "    model_number: {\n",
    "        '制造商': makers,\n",
    "        'FP64': fp64_compute_power,\n",
    "        'FP32': fp32_compute_power,\n",
    "        'TF32': tf32_compute_power,\n",
    "        'FP16': fp16_compute_power,\n",
    "        'INT8': int8_compute_power,\n",
    "        '显存大小': memory_size,\n",
    "        '显存带宽': memory_bandwidth,\n",
    "        '卡间互连': inter_card_interconnect,\n",
    "    },\n",
    "}\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON文件已生成：gpu_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "''''\n",
    "制造商 \\n\n",
    "架构 \\n\n",
    "FP64 \\n\n",
    "FP64 Tensor Core\n",
    "FP32\t\\n\n",
    "TF32 Tensor Core\t\\n\n",
    "BFLOAT16 Tensor Core\t\\n\n",
    "FP16 Tensor Core\t\\n\n",
    "INT8 Tensor Core\t\\n\n",
    "INT4 Tensor Core\t\\n\n",
    "显存大小 | 类型\t\\n\n",
    "显存带宽\t\\n\n",
    "接口类型\t\\n\n",
    "卡间互连\t\\n\n",
    "TDP\t\\n\n",
    "'''\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('computing_gpu.csv', encoding='utf-8')\n",
    "\n",
    "# 创建一个字典来存储结果\n",
    "result = {}\n",
    "\n",
    "# 遍历DataFrame的每一行\n",
    "for _, row in df.iterrows():\n",
    "    model_number = row['算力卡型号']\n",
    "    result[model_number] = {\n",
    "        '制造商': row['制造商'] if pd.notna(row['制造商']) else '-',\n",
    "        '架构': row['架构'] if pd.notna(row['架构']) else '-',\n",
    "        'FP64': row['FP64'] if pd.notna(row['FP64']) else '-',\n",
    "        'FP64 Tensor Core': row['FP64 Tensor Core'] if pd.notna(row['FP64 Tensor Core']) else '-',\n",
    "        'FP32': row['FP32'] if pd.notna(row['FP32']) else '-',\n",
    "        'TF32 Tensor Core': row['TF32 Tensor Core'] if pd.notna(row['TF32 Tensor Core']) else '-',\n",
    "        'BFLOAT16 Tensor Core': row['BFLOAT16 Tensor Core'] if pd.notna(row['BFLOAT16 Tensor Core']) else '-',\n",
    "        'FP16 Tensor Core': row['FP16 Tensor Core'] if pd.notna(row['FP16 Tensor Core']) else '-',\n",
    "        'INT8 Tensor Core': row['INT8 Tensor Core'] if pd.notna(row['INT8 Tensor Core']) else '-',\n",
    "        'INT4 Tensor Core': row['INT4 Tensor Core'] if pd.notna(row['INT4 Tensor Core']) else '-',\n",
    "        '显存大小 | 类型': row['显存大小 | 类型'] if pd.notna(row['显存大小 | 类型']) else '-',\n",
    "        '显存带宽': row['显存带宽'] if pd.notna(row['显存带宽']) else '-',\n",
    "        '接口类型': row['接口类型'] if pd.notna(row['接口类型']) else '-',\n",
    "        '卡间互连': row['卡间互连'] if pd.notna(row['卡间互连']) else '-',\n",
    "        'TDP': row['TDP'] if pd.notna(row['TDP']) else '-',\n",
    "    }\n",
    "\n",
    "# 将结果转换为JSON字符串\n",
    "json_output = json.dumps(result, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 将JSON字符串写入文件\n",
    "with open('gpu_data.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json_output)\n",
    "\n",
    "print(\"JSON文件已生成：gpu_data.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
