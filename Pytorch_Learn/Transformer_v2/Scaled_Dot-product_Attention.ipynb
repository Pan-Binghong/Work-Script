{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scaled Dot-product Attention\n",
    "代码实现了Scaled Dot-product Attention，并进行了测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many parameters - -A\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7592, 2088]])\n",
      "Embedding(30522, 768)\n",
      "torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "from torch import nn  # 导入PyTorch的nn模块，用于定义神经网络层\n",
    "from transformers import AutoConfig  # 导入自动配置类，用于获取预训练模型的配置\n",
    "from transformers import AutoTokenizer  # 导入自动分词器类，用于文本的分词和编码\n",
    "\n",
    "# 指定预训练的BERT模型\n",
    "cache_dir = './pretrained_model'\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=cache_dir)  # 从预训练模型加载分词器\n",
    "\n",
    "# 准备输入文本\n",
    "text = \"hello world\"\n",
    "\n",
    "# 使用分词器处理文本，返回特殊的tensor格式\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)  # 不添加特殊标记\n",
    "print(inputs.input_ids)  # 打印输入的token ids\n",
    "\n",
    "# 获取模型的配置\n",
    "config = AutoConfig.from_pretrained(model_ckpt)  # 从预训练模型加载配置\n",
    "\n",
    "# 创建一个嵌入层，用于将token ids转换为词向量\n",
    "# vocab_size是词汇表的大小，hidden_size是嵌入向量的维度\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "print(token_emb)  # 打印嵌入层\n",
    "\n",
    "# 将输入的token ids通过嵌入层转换为词向量\n",
    "inputs_embeds = token_emb(inputs.input_ids)  # 调用嵌入层的forward方法\n",
    "print(inputs_embeds.size())  # 打印词向量的尺寸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[25.9001, -0.7132],\n",
      "         [-0.7132, 25.8847]]], grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch  # 导入PyTorch库，用于进行张量运算\n",
    "from math import sqrt  # 导入math库中的sqrt函数，用于计算平方根\n",
    "\n",
    "# 假设inputs_embeds是之前步骤中得到的词嵌入向量，这里同时作为查询（Q）、键（K）和值（V）\n",
    "Q = K = V = inputs_embeds  # 这里的Q, K, V是相同的词嵌入向量，但在实际应用中它们可能来自不同的输入\n",
    "\n",
    "# 获取键（K）的维度大小，即键的嵌入维度\n",
    "dim_k = K.size(-1)  # -1表示最后一个维度，这里是嵌入向量的维度\n",
    "\n",
    "# 计算注意力分数\n",
    "'''\n",
    "为了计算Q和K的点积，我们需要调整K的维度，以便它与Q的维度兼容。原始的K矩阵具有形状(batch_size, sent_len, emb_dim)，\n",
    "为了执行矩阵乘法，我们需要将K的第二个和第三个维度交换，这里我们可以使用转置操作transpose(1, 2)将K的维度变为(batch_size, emb_dim, sent_len)。\n",
    "\n",
    "现在，Q和K的形状都是(batch_size, sent_len, emb_dim)，我们可以执行矩阵乘法。\n",
    "\n",
    "Q 的形状：(batch_size, sent_len, emb_dim)\n",
    "\n",
    "K 的转置的形状：(batch_size, emb_dim, sent_len)\n",
    "\n",
    "注意力分数的形状：(batch_size, sent_len, sent_len)\n",
    "'''\n",
    "# 每个分数表示一个查询向量与所有键向量之间的相似度\n",
    "scores = torch.bmm(Q, K.transpose(1, 2)) / sqrt(dim_k)  # 计算得到的分数矩阵并缩放\n",
    "print(scores)  # 打印\n",
    "# 这个矩阵中的每个元素表示一个查询向量与一个键向量之间的相似度分数。\n",
    "\n",
    "# 打印注意力分数的形状\n",
    "print(scores.size())  # 应该输出(batch_size, sent_len, sent_len)\n",
    "# 具体来说，scores 的每个元素 scores[b, i, j] 表示第 b 个批次中第 i 个查询向量与第 j 个键向量之间的相似度分数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000e+00, 2.7667e-12],\n",
      "         [2.8098e-12, 1.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 1.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F  # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数\n",
    "\n",
    "# 使用softmax函数对注意力分数进行归一化，得到注意力权重\n",
    "# softmax函数将查询向量与键向量的相似度分数转换成一个概率分布。\n",
    "# 这里的dim=-1指定了softmax操作的维度，即沿着最后一个维度（sent_len）进行，这个维度表示的是键向量的索引。\n",
    "'''\n",
    "简单讲下，假设\n",
    "scores = [[[1.0, 2.0, 3.0],\n",
    "           [4.0, 5.0, 6.0],\n",
    "           [7.0, 8.0, 9.0]]]\n",
    "1.计算每个元素的指数值：\n",
    "exp_scores = [[[exp(1.0), exp(2.0), exp(3.0)],\n",
    "               [exp(4.0), exp(5.0), exp(6.0)],\n",
    "               [exp(7.0), exp(8.0), exp(9.0)]]]\n",
    "2.计算每个查询向量的指数值之和：\n",
    "sum_exp_scores = [[exp(1.0) + exp(2.0) + exp(3.0),\n",
    "                   exp(4.0) + exp(5.0) + exp(6.0),\n",
    "                   exp(7.0) + exp(8.0) + exp(9.0)]]\n",
    "3.计算 softmax 值：\n",
    "softmax_scores = [[[exp(1.0) / sum_exp_scores[0][0], exp(2.0) / sum_exp_scores[0][0], exp(3.0) / sum_exp_scores[0][0]],\n",
    "                    [exp(4.0) / sum_exp_scores[0][1], exp(5.0) / sum_exp_scores[0][1], exp(6.0) / sum_exp_scores[0][1]],\n",
    "                    [exp(7.0) / sum_exp_scores[0][2], exp(8.0) / sum_exp_scores[0][2], exp(9.0) / sum_exp_scores[0][2]]]]\n",
    "\n",
    "至于为什么不能在查询向量上面做，主要是因为查询向量和键向量的作用不同。查询向量（Q）用于表示我们想要获取信息的请求，而键向量（K）用于表示与查询向量进行比较的键。\n",
    "'''\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "print(weights)  # 打印权重\n",
    "# 打印权重的和，dim=-1表示沿着最后一个维度（即每个词的权重和）进行求和\n",
    "# 由于softmax函数的输出是概率分布，每个维度的和应该接近1（如果不是1，可能是由于浮点数精度问题）\n",
    "print(weights.sum(dim=-1))  # 打印每个查询词的权重和，理论上应该接近1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1071, -1.8374, -0.2551,  ..., -0.2630,  0.3523,  0.1242],\n",
      "         [-0.8179,  0.8697, -1.4724,  ...,  0.6556,  0.3608,  0.0020]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "attn_outputs = torch.bmm(weights, V)\n",
    "print(attn_outputs)\n",
    "print(attn_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-product Attention代码整合/封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "import torch.nn.functional as F  # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数\n",
    "from math import sqrt  # 导入math库中的sqrt函数，用于计算平方根\n",
    "\n",
    "# 定义Scaled Dot-product Attention函数\n",
    "def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None):\n",
    "    # 获取查询（query）的最后一个维度大小，即键（key）的维度\n",
    "    dim_k = query.size(-1)\n",
    "    \n",
    "    # 计算查询和键的点积，并缩放，得到未归一化的注意力分数\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    \n",
    "    # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵\n",
    "    if query_mask is not None and key_mask is not None:\n",
    "        mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1))\n",
    "    else:\n",
    "        # 如果没有提供掩码，则使用之前传入的掩码（如果有的话）\n",
    "        mask = mask\n",
    "    \n",
    "    # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷\n",
    "    # 这样在应用softmax时，这些位置的权重会接近于0\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -float(\"inf\"))\n",
    "    \n",
    "    # 使用softmax函数对分数进行归一化，得到注意力权重\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 计算加权后的输出，即将注意力权重与值（value）相乘\n",
    "    # 这里的输出是经过注意力加权后的值向量，用于下游任务\n",
    "    return torch.bmm(weights, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义AttentionHead类，继承自nn.Module\n",
    "class AttentionHead(nn.Module):\n",
    "    # 初始化函数\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()  # 调用基类的初始化方法\n",
    "        # 定义线性层，用于将输入的词嵌入向量转换为查询（q）、键（k）和值（v）向量\n",
    "        # embed_dim是输入嵌入的维度，head_dim是每个头输出的维度\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None):\n",
    "        # 调用scaled_dot_product_attention函数，传入通过线性层转换后的查询、键和值\n",
    "        # 同时传入可选的掩码参数\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(query),  # 经过查询线性层转换的query\n",
    "            self.k(key),     # 经过键线性层转换的key\n",
    "            self.v(value),   # 经过值线性层转换的value\n",
    "            query_mask,      # 查询掩码\n",
    "            key_mask,        # 键掩码\n",
    "            mask             # 已有的掩码（如果有的话）\n",
    "        )\n",
    "        # 返回注意力机制的输出\n",
    "        return attn_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义MultiHeadAttention类，继承自nn.Module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # 初始化函数\n",
    "    def __init__(self, config):\n",
    "        super().__init__()  # 调用基类的初始化方法\n",
    "        # 从配置中获取嵌入维度和注意力头的数量\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        # 计算每个头的维度大小\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # 创建一个包含多个AttentionHead模块的列表\n",
    "        # 每个头都使用相同的嵌入维度和头维度\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        # 定义输出线性层，用于将多头注意力的输出合并\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None):\n",
    "        # 并行通过每个注意力头处理输入\n",
    "        # 使用torch.cat将所有头的输出在最后一个维度上拼接起来\n",
    "        x = torch.cat([\n",
    "            h(query, key, value, query_mask, key_mask, mask) for h in self.heads\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # 通过输出线性层处理拼接后的输出\n",
    "        x = self.output_linear(x)\n",
    "        \n",
    "        # 返回最终的输出\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "cache_dir = \"./pretrained_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=cache_dir)\n",
    "\n",
    "text = \"hello world\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "\n",
    "multihead_attn = MultiHeadAttention(config)\n",
    "query = key = value = inputs_embeds\n",
    "attn_output = multihead_attn(query, key, value)\n",
    "print(attn_output.size()) #torch.Size([1, 5, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义FeedForward类，继承自nn.Module\n",
    "class FeedForward(nn.Module):\n",
    "    # 初始化函数\n",
    "    def __init__(self, config):\n",
    "        super().__init__()  # 调用基类的初始化方法\n",
    "        # 定义第一个线性层，将输入的隐藏状态映射到中间维度\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        # 定义第二个线性层，将中间维度的表示映射回原始的隐藏状态维度\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        # 定义GELU激活函数\n",
    "        self.gelu = nn.GELU()\n",
    "        # 定义Dropout层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 应用第一个线性层\n",
    "        x = self.linear_1(x)\n",
    "        # 应用GELU激活函数\n",
    "        x = self.gelu(x)\n",
    "        # 应用第二个线性层\n",
    "        x = self.linear_2(x)\n",
    "        # 应用Dropout\n",
    "        x = self.dropout(x)\n",
    "        # 返回最终的输出\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义TransformerEncoderLayer类，继承自nn.Module\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    # 初始化函数\n",
    "    def __init__(self, config):\n",
    "        super().__init__()  # 调用基类的初始化方法\n",
    "        # 定义第一个层归一化，用于注意力机制之前\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        # 定义第二个层归一化，用于前馈网络之前\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        # 定义多头注意力机制\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        # 定义前馈神经网络\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x, mask=None):\n",
    "        # 应用第一个层归一化\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # 应用注意力机制，并将结果与输入进行残差连接\n",
    "        # 注意力机制的输出将与输入x相加，得到更新后的x\n",
    "        x = x + self.attention(hidden_state, hidden_state, hidden_state, mask=mask)\n",
    "        # 应用第二个层归一化\n",
    "        # 注意这里的self.layer_norm_2(x)实际上是对更新后的x进行归一化\n",
    "        hidden_state = self.layer_norm_2(x)\n",
    "        # 应用前馈网络，并将结果与更新后的x进行残差连接\n",
    "        x = x + self.feed_forward(hidden_state)\n",
    "        # 返回最终的输出x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "print(inputs_embeds.shape)\n",
    "print(encoder_layer(inputs_embeds).size())\n",
    "#torch.Size([1, 5, 768])\n",
    "#torch.Size([1, 5, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绝对位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, LongTensor, arange\n",
    "\n",
    "# 定义Embeddings类，继承自nn.Module\n",
    "class Embeddings(nn.Module):\n",
    "    # 初始化函数\n",
    "    def __init__(self, config):\n",
    "        super().__init__()  # 调用基类的初始化方法\n",
    "        # 定义词嵌入层，将词ID映射到词向量\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        # 定义位置嵌入层，为序列中的每个位置生成一个唯一的位置向量\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        # 定义层归一化，用于稳定训练过程\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        # 定义Dropout层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, input_ids):\n",
    "        # 根据输入序列的长度创建位置ID\n",
    "        seq_length = input_ids.size(1)  # 获取序列长度\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)  # 创建位置ID序列\n",
    "        # 创建词嵌入和位置嵌入\n",
    "        token_embeddings = self.token_embeddings(input_ids)  # 通过词嵌入层获取词嵌入\n",
    "        position_embeddings = self.position_embeddings(position_ids)  # 通过位置嵌入层获取位置嵌入\n",
    "        # 将词嵌入和位置嵌入相加，得到最终的嵌入表示\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        # 应用层归一化\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        # 应用Dropout\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        # 返回最终的嵌入表示\n",
    "        return embeddings\n",
    "\n",
    "# 创建Embeddings层的实例，并使用config配置\n",
    "embedding_layer = Embeddings(config)\n",
    "\n",
    "# 使用embedding_layer处理输入的词ID，并打印输出的大小\n",
    "# 这里假设inputs.input_ids是之前通过tokenizer得到的词ID序列\n",
    "print(embedding_layer(inputs.input_ids).size()) #torch.Size([1, 5, 768])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
